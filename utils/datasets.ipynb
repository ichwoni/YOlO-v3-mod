{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from utils.augmentations import horisontal_flip\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def pad_to_square(img, pad_value):\n",
        "    c, h, w = img.shape\n",
        "    dim_diff = np.abs(h - w)\n",
        "    # (upper / left) padding and (lower / right) padding\n",
        "    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
        "    # Determine padding\n",
        "    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n",
        "    # Add padding\n",
        "    img = F.pad(img, pad, \"constant\", value=pad_value)\n",
        "\n",
        "    return img, pad\n",
        "\n",
        "\n",
        "def resize(image, size):\n",
        "    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_resize(images, min_size=288, max_size=448):\n",
        "    new_size = random.sample(list(range(min_size, max_size + 1, 32)), 1)[0]\n",
        "    images = F.interpolate(images, size=new_size, mode=\"nearest\")\n",
        "    return images\n",
        "\n",
        "\n",
        "class ImageFolder(Dataset):\n",
        "    def __init__(self, folder_path, img_size=416):\n",
        "        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[index % len(self.files)]\n",
        "        # Extract image as PyTorch tensor\n",
        "        img = transforms.ToTensor()(Image.open(img_path))\n",
        "        # Pad to square resolution\n",
        "        img, _ = pad_to_square(img, 0)\n",
        "        # Resize\n",
        "        img = resize(img, self.img_size)\n",
        "\n",
        "        return img_path, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    def __init__(self, list_path, img_size=416, augment=True, multiscale=True, normalized_labels=True):\n",
        "        with open(list_path, \"r\") as file:\n",
        "            self.img_files = file.readlines()\n",
        "\n",
        "        self.label_files = [\n",
        "            path.replace(\"images\", \"labels\").replace(\".png\", \".txt\").replace(\".jpg\", \".txt\")\n",
        "            for path in self.img_files\n",
        "        ]\n",
        "        self.img_size = img_size\n",
        "        self.max_objects = 100\n",
        "        self.augment = augment\n",
        "        self.multiscale = multiscale\n",
        "        self.normalized_labels = normalized_labels\n",
        "        self.min_size = self.img_size - 3 * 32\n",
        "        self.max_size = self.img_size + 3 * 32\n",
        "        self.batch_count = 0\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # ---------\n",
        "        #  Image\n",
        "        # ---------\n",
        "\n",
        "        img_path = self.img_files[index % len(self.img_files)].rstrip()\n",
        "\n",
        "        # Extract image as PyTorch tensor\n",
        "        img = transforms.ToTensor()(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "        # Handle images with less than three channels\n",
        "        if len(img.shape) != 3:\n",
        "            img = img.unsqueeze(0)\n",
        "            img = img.expand((3, img.shape[1:]))\n",
        "\n",
        "        _, h, w = img.shape\n",
        "        h_factor, w_factor = (h, w) if self.normalized_labels else (1, 1)\n",
        "        # Pad to square resolution\n",
        "        img, pad = pad_to_square(img, 0)\n",
        "        _, padded_h, padded_w = img.shape\n",
        "\n",
        "        # ---------\n",
        "        #  Label\n",
        "        # ---------\n",
        "\n",
        "        label_path = self.label_files[index % len(self.img_files)].rstrip()\n",
        "\n",
        "        targets = None\n",
        "        if os.path.exists(label_path):\n",
        "            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n",
        "            # Extract coordinates for unpadded + unscaled image\n",
        "            x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\n",
        "            y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\n",
        "            x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\n",
        "            y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\n",
        "            # Adjust for added padding\n",
        "            x1 += pad[0]\n",
        "            y1 += pad[2]\n",
        "            x2 += pad[1]\n",
        "            y2 += pad[3]\n",
        "            # Returns (x, y, w, h)\n",
        "            boxes[:, 1] = ((x1 + x2) / 2) / padded_w\n",
        "            boxes[:, 2] = ((y1 + y2) / 2) / padded_h\n",
        "            boxes[:, 3] *= w_factor / padded_w\n",
        "            boxes[:, 4] *= h_factor / padded_h\n",
        "\n",
        "            targets = torch.zeros((len(boxes), 6))\n",
        "            targets[:, 1:] = boxes\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.augment:\n",
        "            if np.random.random() < 0.5:\n",
        "                img, targets = horisontal_flip(img, targets)\n",
        "\n",
        "        return img_path, img, targets\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        paths, imgs, targets = list(zip(*batch))\n",
        "        # Remove empty placeholder targets\n",
        "        targets = [boxes for boxes in targets if boxes is not None]\n",
        "        # Add sample index to targets\n",
        "        for i, boxes in enumerate(targets):\n",
        "            boxes[:, 0] = i\n",
        "        targets = torch.cat(targets, 0)\n",
        "        # Selects new image size every tenth batch\n",
        "        if self.multiscale and self.batch_count % 10 == 0:\n",
        "            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n",
        "        # Resize images to input shape\n",
        "        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n",
        "        self.batch_count += 1\n",
        "        return paths, imgs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}